# Machine Learning Projects

This repository contains a collection of machine learning models for various tasks, including classification and regression.

---

## Project Description Document

Below is a summary of the models implemented in this project, following the specified documentation structure.

### 1. Logistic Regression Model (Classification)

*   **a. General Information on Dataset:**
    *   **Name:** Fashion-MNIST
    *   **Number of Classes:** 5 (after removing 5 classes with the lowest accuracy)
    *   **Labels:** `1: Trouser`, `3: Dress`, `5: Sandal`, `8: Bag`, `9: Ankle boot`
    *   **Total Samples:** 36,000 (after removing classes)
    *   **Training/Validation/Testing Split:** 60,000 samples were used for training and 10,000 for testing. During development, the training set was split 80/20 for training and validation. After removing classes, the dataset was reduced.
    *   **Size of Each Sample:** 28x28 pixel grayscale images.

*   **b. Implementation Details:**
    *   **Feature Extraction:** The initial 784 pixel values were transformed using **Histogram of Oriented Gradients (HOG)** to extract important features. Outliers in the HOG features were then capped using the IQR method. The features were then scaled using `StandardScaler`. Dimensionality was then reduced to **180 principal components** using PCA.
    *   **Cross-validation:** Yes, **5-fold cross-validation** was performed on the training data.
    *   **Hyperparameters:**
        *   **Model:** A `Pipeline` combining `StandardScaler`, `PCA`, and `LogisticRegression`.
        *   **PCA `n_components`:** 180
        *   **Optimizer (`solver`):** `saga`
        *   **Regularization:** L1
        *   **`C`:** 0.1
        *   **`max_iter`:** 2000

*   **c. Results Details (on Testing Data):**
    *   **Note:** The results below are not up-to-date after the removal of classes and need to be regenerated by running the notebook.
    *   **Accuracy:** 82.95%
    *   **Confusion Matrix:** A detailed confusion matrix was generated to show per-class performance.
    *   **ROC Curve:** ROC curves were plotted for each of the 5 classes.
    *   **Loss Curve:** A loss curve was not generated for this model.

---

### 2. K-Means Clustering Model (Clustering)

*   **a. General Information on Dataset:**
    *   Same as the Logistic Regression model (Fashion-MNIST, with 5 classes).

*   **b. Implementation Details:**
    *   **Feature Extraction:** Same as the Logistic Regression model (HOG, outlier capping, `StandardScaler` followed by `PCA` with 180 components).
    *   **Cross-validation:** Not used for this unsupervised model.
    *   **Hyperparameters:**
        *   **Model:** A `Pipeline` combining `StandardScaler`, `PCA`, and `KMeans`.
        *   **PCA `n_components`:** 180
        *   **`n_clusters`:** 5
        *   **`random_state`:** 42

*   **c. Results Details:**
    *   **Note:** The results below are not up-to-date after the removal of classes and need to be regenerated by running the notebook.
    *   As an unsupervised model, standard classification metrics do not apply. Performance was evaluated using clustering metrics on the test set:
    *   **Adjusted Rand Index (ARI):** 0.333
    *   **Normalized Mutual Information (NMI):** 0.486
    *   **Loss Curve, Accuracy, Confusion Matrix, ROC Curve:** Not applicable for this task.

---

### 3. Linear Regression Model (Regression)

*   **a. General Information on Dataset:**
    *   **Name:** Insurance Charges Prediction Dataset
    *   **Total Samples:** 1,199 (after removing outliers).
    *   **Training/Testing Split:** 80% for training, 20% for testing.

*   **b. Implementation Details:**
    *   **Refactoring:** The model has been refactored from a Jupyter Notebook to a Python script (`src/Numerical/linear_regression_model.py`) for better modularity and reusability.
    *   **Feature Extraction:** 5 features were used: `age`, `sex`, `bmi`, `children`, and `smoker`. Outliers in the `charges` column were removed using the IQR method.
    *   **Cross-validation:** Not used.
    *   **Hyperparameters:** A standard `LinearRegression` model with default parameters was used.

*   **c. Results Details (on Testing Data):**
    *   **R-squared (R²) Score:** **0.524** (The model explains ~52.4% of the variance in the charges).
    *   **Root Mean Squared Error (RMSE):** 5,364.79
    *   **Plots:** The analysis includes an "actual vs. prediction" scatter plot and an error distribution histogram.
    *   **Loss Curve, Accuracy, Confusion Matrix, ROC Curve:** Not applicable for this regression task.

---

### 4. K-Nearest Neighbors (KNN) Regressor Model (Regression)

*   **a. General Information on Dataset:**
    *   **Name:** Insurance Charges Prediction Dataset
    *   **Total Samples:** 1,338
    *   **Training/Testing Split:** 80% for training, 20% for testing.

*   **b. Implementation Details:**
    *   **Refactoring:** The model has been refactored from a Jupyter Notebook to a Python script (`src/Numerical/knn_regressor_model.py`) for better modularity and reusability.
    *   **Feature Extraction:** 8 features were used after one-hot encoding categorical variables (`sex`, `smoker`, `region`). All features were then scaled using `StandardScaler`.
    *   **Cross-validation:** Not used.
    *   **Hyperparameters:** `KNeighborsRegressor` was used with `n_neighbors=5`.

*   **c. Results Details (on Testing Data):**
    *   **R-squared (R²) Score:** **0.804** (The model explains ~80.4% of the variance in the charges).
    *   **Root Mean Squared Error (RMSE):** 5,519.05
    *   **Loss Curve:** A learning curve showing MSE loss versus training size was generated.
    *   **Plots:** The analysis includes a residual plot and an "actual vs. predicted" plot.
    *   **Accuracy, Confusion Matrix, ROC Curve:** Not applicable for this regression task.